---
---

@inproceedings{10.1145/3539618.3591934,
author = {Lee, Hyunsung and Yoo, Sungwook and Lee, Dongjun and Kim, Jaekwang},
title = {How Important is Periodic Model Update in Recommender System?},
year = {2023},
isbn = {9781450394086},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3539618.3591934},
doi = {10.1145/3539618.3591934},
abstract = {In real-world recommender model deployments, the models are typically retrained and deployed repeatedly. It is the rule-of-thumb to periodically retrain recommender models to capture up-to-date user behavior and item trends. However, the harm caused by delayed model updates has not been investigated extensively yet. in this perspective paper, we formulate the delayed model update problem and quantitatively demonstrate the delayed model update actually harms the model performance by increasing the number of cold users and cold items increase and decreasing overall model performances. These effects vary across different domains having different characteristics. Upon these findings, we further argue that although the delayed model update has negative effects on online recommender model deployment, yet it has not gathered enough attention from research communities. We argue our verification of the relationship between the model update cycle and model performance calls for further research such as faster model training, and more efficient data pipelines to keep the model more up-to-date with the latest user behaviors and item trends.},
booktitle = {Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2661–2668},
numpages = {8},
keywords = {recommender systems, model retraining, delayed model update},
location = {Taipei, Taiwan},
series = {SIGIR '23},
selected={true},
preview={periodic_update.png}
}

@inproceedings{10.1145/3583780.3615184,
author = {Ko, Donggeun and Lee, Dongjun and Park, Namjun and Noh, Kyoungrae and Park, Hyeonjin and Kim, Jaekwang},
title = {AmpliBias: Mitigating Dataset Bias through Bias Amplification in Few-Shot Learning for Generative Models},
year = {2023},
isbn = {9798400701245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583780.3615184},
doi = {10.1145/3583780.3615184},
abstract = {Deep learning models exhibit a dependency on peripheral attributes of input data, such as shapes and colors, leading the models to become biased towards these certain attributes that result in subsequent degradation of performance. In this paper, we alleviate this problem by presenting~sysname, a novel framework that tackles dataset bias by leveraging generative models to amplify bias and facilitate the learning of debiased representations of the classifier. Our method involves three major steps. We initially train a biased classifier, denoted as f_b, on a biased dataset and extract the top-K biased-conflict samples. Next, we train a generator solely on a bias-conflict dataset comprised of these top-K samples, aiming to learn the distribution of bias-conflict samples. Finally, we re-train the classifier on the newly constructed debiased dataset, which combines the original and amplified data. This allows the biased classifier to competently learn debiased representation. Extensive experiments validate that our proposed method effectively debiases the biased classifier.},
booktitle = {Proceedings of the 32nd ACM International Conference on Information and Knowledge Management},
pages = {4028–4032},
numpages = {5},
keywords = {debiasing, few-shot learning, generative model},
location = {Birmingham, United Kingdom},
series = {CIKM '23},
selected={true},
preview={AmpliBias.png}
}