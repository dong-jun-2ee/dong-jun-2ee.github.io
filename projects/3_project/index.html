<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Korean Reasoning Model | Dongjun Lee </title> <meta name="author" content="Dongjun Lee"> <meta name="description" content="Training an o1-like large language model capable of reasoning and responding in Korean."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://dong-jun-2ee.github.io/projects/3_project/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Dongjun</span> Lee </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Korean Reasoning Model</h1> <p class="post-description">Training an o1-like large language model capable of reasoning and responding in Korean.</p> </header> <article> <h2 id="contents">Contents</h2> <ul> <li><a href="#summary">Summary</a></li> <li> <a href="#training-pipeline">Training Pipeline</a> <ul> <li><a href="#building-datasets">Building Datasets</a></li> <li><a href="#knowledge-distillation">Knowledge Distillation</a></li> <li><a href="#reward-modeling">Reward Modeling</a></li> <li><a href="#reinforcement-learning">Reinforcement Learning</a></li> </ul> </li> <li> <a href="#results">Results</a> <ul> <li><a href="#reward-model">Reward Model</a></li> <li><a href="#math">Math</a></li> <li><a href="#examples">Examples</a></li> </ul> </li> <li><a href="#discussion">Discussion</a></li> </ul> <p><br></p> <hr> <h2 id="summary">Summary</h2> <p>Recently, models equipped with reasoning capabilities have demonstrated remarkable performance, such as <a href="https://arxiv.org/abs/2412.16720" rel="external nofollow noopener" target="_blank">ChatGPT-o1</a>.</p> <p>In the open research community, <a href="https://arxiv.org/abs/2501.12948" rel="external nofollow noopener" target="_blank">Deepseek-AI</a> provides recipes for building <code class="language-plaintext highlighter-rouge">o1-like models</code>, paving the way to equip models with reasoning capabilities.</p> <p>Similarly, using the <a href="https://github.com/volcengine/verl" rel="external nofollow noopener" target="_blank">verl</a> repository, we train a large language model that generates its own reasoning process to improve performance. In particular, unlike the open o1-like models that support only English responses, we align our model to generate both reasoning traces and final summaries (i.e., primarily tokens without mathematical notation or code) in <code class="language-plaintext highlighter-rouge">Korean</code>.</p> <p>We achieve performance improvements of up to <strong><u>18.67%</u></strong> points over the original model, despite constraints on language consistency.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/k-reasoning-480.webp 480w,/assets/img/k-reasoning-800.webp 800w,/assets/img/k-reasoning-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/k-reasoning.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="k-reasoning" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Overall pipeline for training the Korean Reasoning Model </div> <p><br></p> <hr> <h2 id="training-pipeline">Training Pipeline</h2> <ul> <li> <p>Building Datasets</p> </li> <li> <p>Knowledge Distillation</p> </li> <li> <p>Reward Modeling</p> </li> <li> <p>Reinforcement Learning</p> </li> </ul> <p><br></p> <h3 id="building-datasets">Building Datasets</h3> <p>Following previous research, we generate prompt-response pairs for math and general conversation from a high-capability reasoning model for the knowledge distillation stage.</p> <p>We generate responses to the same prompts from multiple models. In addition, we improve the quality of the data through outcome-based filtering and generative rewards, collecting <code class="language-plaintext highlighter-rouge">approximately 200k data samples</code> for knowledge distillation process.</p> <p><br></p> <h3 id="knowledge-distillation">Knowledge Distillation</h3> <p>We train the model to generate reasoning trajectories using <a href="https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct" rel="external nofollow noopener" target="_blank">Qwen2.5-1.5B-Instruct</a> for this work. This model is chosen due to its strong performance in both Korean and English, as well as its permissive commercial use.</p> <p>Using the prompt-response pairs generated in the previous stage, we perform black-box distillation, which enable the model to learn how to generate thoughts from cold-start data.</p> <p><br></p> <h3 id="reward-modeling">Reward Modeling</h3> <p>To assign rewards for general conversation, we build a reward model that supports Korean. This process mainly follows the pipeline of <a href="https://arxiv.org/abs/2410.18451" rel="external nofollow noopener" target="_blank">Skyword-Reward</a>, and we use publicly available models that support Korean to generate Korean response (chosen, rejected) pairs (e.g., <a href="https://huggingface.co/mistralai/Mistral-Small-24B-Instruct-2501" rel="external nofollow noopener" target="_blank">Mistral</a>, <a href="https://huggingface.co/Qwen/Qwen2.5-32B-Instruct" rel="external nofollow noopener" target="_blank">Qwen</a>).</p> <p>Based on the generated responses, we assign rewards using three open reward models. After performing re-centering based on the performance of each model, we construct three types of <code class="language-plaintext highlighter-rouge">preference datasets</code> (chosen, rejected pairs).</p> <p>We train the reward model using each type of dataset, and select the final model by evaluating its performance on <a href="https://arxiv.org/abs/2403.13787" rel="external nofollow noopener" target="_blank">RewardBench</a> and <a href="https://arxiv.org/abs/2410.15522" rel="external nofollow noopener" target="_blank">Multilingual Reward Bench</a>.</p> <p><br></p> <h3 id="reinforcement-learning">Reinforcement Learning</h3> <p>At this stage, our goal is to endow the chat model obtained from the knowledge distillation phase with reasoning capability. Specifically, we train the model on math data using outcome-based rewards, which have been shown in previous studies to contribute significantly to performance improvement. Similar to <code class="language-plaintext highlighter-rouge">Deepseekai-R1</code>, we apply the reward model to summaries only.</p> <p>Unlike previous successful reproductions, we additionally apply a language consistency reward, focusing on aligning the language used between the input prompt and the generated thought. One of the key insights from this process is that removing code snippets, mathematical equations, and markdown elements is crucial for preventing reward hacking.</p> <p>We are able to accomplish this task by applying the <a href="https://arxiv.org/abs/2402.03300" rel="external nofollow noopener" target="_blank">GRPO</a> reinforcement learning method.</p> <p><br></p> <hr> <h2 id="results">Results</h2> <p>In this section, we present the performance evaluation of the trained reward model and the fine-tuned final reasoner on the reward benchmark datasets and mathematical datasets, respectively.</p> <p><br></p> <h3 id="reward-model">Reward Model</h3> <table> <thead> <tr> <th>Models</th> <th>Chat</th> <th>Chat Hard</th> <th>Safety</th> <th>Reasoning</th> <th>Overall</th> <th>w/o Reasoning</th> <th>M-Reward Bench</th> </tr> </thead> <tbody> <tr> <td>GPT-4o-0513</td> <td>96.60%</td> <td>70.40%</td> <td>86.70%</td> <td>84.90%</td> <td>85.37%</td> <td>85.98%</td> <td>-</td> </tr> <tr> <td>GPT-4o-0806</td> <td>96.10%</td> <td>76.10%</td> <td>88.10%</td> <td>86.60%</td> <td>87.12%</td> <td>87.80%</td> <td>-</td> </tr> <tr> <td>GPT-4o-1120</td> <td>95.80%</td> <td>71.30%</td> <td>86.20%</td> <td>85.20%</td> <td>85.41%</td> <td>85.71%</td> <td>-</td> </tr> <tr> <td>Claude-3.5-sonnet-0620</td> <td>96.40%</td> <td>74.00%</td> <td>81.60%</td> <td>84.70%</td> <td>84.58%</td> <td>84.48%</td> <td>-</td> </tr> <tr> <td>Claude-3.5-sonnet-1022</td> <td>96.40%</td> <td>79.70%</td> <td>91.10%</td> <td>87.60%</td> <td>88.70%</td> <td>90.09%</td> <td>-</td> </tr> <tr> <td>DeepSeek-V3</td> <td>96.90%</td> <td>79.80%</td> <td>87.00%</td> <td>84.30%</td> <td>86.15%</td> <td>88.43%</td> <td>-</td> </tr> <tr> <td>GPT-4 Turbo</td> <td>-</td> <td>-</td> <td>-</td> <td>-</td> <td>-</td> <td>-</td> <td>82.50%</td> </tr> <tr> <td>GPT-4o</td> <td>-</td> <td>-</td> <td>-</td> <td>-</td> <td>-</td> <td>-</td> <td>79.20%</td> </tr> <tr> <td>Nemotron-4-340B-Reward</td> <td>95.80%</td> <td><strong>87.10%</strong></td> <td>91.50%</td> <td><strong>93.70%</strong></td> <td><strong>92.83%</strong></td> <td><strong>91.83%</strong></td> <td>-</td> </tr> <tr> <td>Gemini-1.5-Pro-0514</td> <td>92.30%</td> <td>80.60%</td> <td>87.50%</td> <td>92.00%</td> <td>89.90%</td> <td>87.39%</td> <td>-</td> </tr> <tr> <td>Meta-Llama-3-70B-Instruct-Self-Taught</td> <td><strong>98.00%</strong></td> <td>80.30%</td> <td>90.50%</td> <td>82.20%</td> <td>85.91%</td> <td>90.46%</td> <td>-</td> </tr> <tr> <td>Meta-Llama-3-70B-RM-Helpsteer2-classifier</td> <td>91.30%</td> <td>80.30%</td> <td><strong>92.80%</strong></td> <td>90.70%</td> <td>90.08%</td> <td>89.38%</td> <td>-</td> </tr> <tr> <td>Ours (w. Reasoning)</td> <td>75.14%</td> <td>87.94%</td> <td>91.62%</td> <td>91.99%</td> <td>88.94%</td> <td>85.55%</td> <td>84.04%</td> </tr> <tr> <td><strong>Ours (w.o Reasoning) </strong></td> <td>94.13%</td> <td>81.58%</td> <td>91.89%</td> <td>78.33%</td> <td>86.47%</td> <td>90.17%</td> <td><strong>84.66%</strong></td> </tr> </tbody> </table> <p><br></p> <p>The performance table shows a linear <code class="language-plaintext highlighter-rouge">positive correlation between model size</code> and reward benchmark performance. Additionally, while closed-source LLMs demonstrate excellent reward performance, there are significant limitations to using them for training. Nevertheless, we observe that the reward benchmark performance for the Korean language remains insufficient. In contrast, our model demonstrates balanced performance in both English and Korean.</p> <p>We note that rewards related to reasoning are excluded from the reward model training data to enable the application of rule-based rewards. In fact, during our experiments, we find that models with <code class="language-plaintext highlighter-rouge">high performance in reasoning tended to show relatively lower results</code> in general conversation and multilingual rewarding tasks.</p> <p><br></p> <h3 id="math">Math</h3> <table> <thead> <tr> <th>Models</th> <th>AIME 2024</th> <th>AMC 2023</th> <th>AMC-ko</th> </tr> </thead> <tbody> <tr> <td>Qwen/Qwen2.5-1.5B-Instruct</td> <td>0.0%</td> <td>25.75%</td> <td>6.02%</td> </tr> <tr> <td>DeepSeek-R1-Distill-Qwen-1.5B (Think in English)</td> <td>20.67%</td> <td>45.75%</td> <td>37.34%</td> </tr> <tr> <td>Ours</td> <td>18.67%</td> <td>35.00%</td> <td>25.63%</td> </tr> </tbody> </table> <p><br><br></p> <p>To evaluate the reasoning capabilities of our model, we utilize benchmark datasets AIME2024 and AME2023. We adopt pass@1 as the evaluation metric, and the baselines for comparison included the <a href="https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct" rel="external nofollow noopener" target="_blank">Qwen/Qwen2.5-1.5B-Instruct</a> and <a href="https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B" rel="external nofollow noopener" target="_blank">DeepSeek-R1-Distill-Qwen-1.5B</a> models.</p> <p>Our fine-tuned model demonstrates the highest performance, showing a remarkable improvement over the base model <code class="language-plaintext highlighter-rouge">Qwen/Qwen2.5-1.5B-Instruct</code>, particularly on the AIME2024 dataset. Furthermore, it achieve performance comparable to that of the DeepSeek-R1-Distill-Qwen-1.5B model. Importantly, unlike the English-only baselines, our model supports reasoning in Korean, which retains significant value.</p> <p>On the Korean-translated AMC2022 and AMC2023 datasets, the Qwen model exhibits limited reasoning ability and achieves low performance. In contrast, DeepSeek model, which is primarily trained on Chinese and English, shows notable performance but is unable to generate responses in Korean. Our model, however, supports Korean input and achieves the compable performance among the models tested. However, it is also capable of generating responses in Korean. Furthermore, a performance gap remains when compared to English inputs, indicating room for further improvement.</p> <p><br></p> <h3 id="examples">Examples</h3> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
### English
Prompt: 
Alice and Bob play the following game. A stack of $n$ tokens lies before them. The players take turns with Alice going first. On each turn, the player removes either $1$ token or $4$ tokens from the stack. Whoever removes the last token wins. Find the number of positive integers $n$ less than or equal to $2024$ for which there exists a strategy for Bob that guarantees that Bob will win the game regardless of Alice's play.

Response: 
&lt;think&gt;Okay, so Alice and Bob are playing this game with a stack of n tokens. They take turns removing either 1 or 4 tokens. Alice goes first. The person who takes the last token wins. I need to find how many positive integers n ≤ 2024 allow Bob to have a winning strategy, no matter what Alice does. Hmm, let's see.\n\nFirst, this seems like a combinatorial game theory problem. I remember that in such games, we often look for winning and losing positions.
...
**Numbers congruent to 0 modulo 5**: These are \\( 5, 10, 15, \\ldots, 2020 \\). The number of such terms is \\( \\left\\lfloor \\frac{2024}{5} \\right\\rfloor = 404 \\).\n2. **Numbers congruent to 2 modulo 5**: These are \\( 2, 7, 12, \\ldots, 2022 \\). The number of such terms is \\( \\left\\lfloor \\frac{2024 - 2}{5} \\right\\rfloor + 1 = 405 \\).\n\nAdding these together, the total number of losing positions is \\( 404 + 405 = 809 \\).\n\nThus, the number of positive integers \\( n \\leq 2024 \\) for which Bob can guarantee a win is \\(\\boxed{809}\\)."
---

---
### Korean
Prompt: 
너는 지금 게임을 하고 있어. $3  imes 3$ 격자판 위에 $2  imes 1$ 직사각형이 인접한 두 칸을 (가로거나 세로로) 덮고 있는데, 어떤 두 칸이 덮였는지는 알려주지 않아. 네 목표는 직사각형이 덮고 있는 칸 중 하나라도 찾아내는 거야. "한 번의 시도"는 네가 어떤 칸을 찍는 걸 의미하고, 찍고 나면 그 칸이 숨겨진 직사각형에 덮여 있는지 알려줘. 직사각형이 덮고 있는 칸 하나를 무조건 찾아내려면 최소 몇 번을 찍어야 할까?

Response: 
좋아, 이 문제를 풀어보자. 문제는 3x3 격자판 위에 2x1 직사각형이 인접한 두 칸을 덮고 있는데, 어떤 두 칸이 덮였는지도 모르니까, 네가 특정 칸을 찍으면 그 칸이 직사각형에 덮여 있는지 확인해야 해. 그리고 이걸 하기 위해 최소 몇 번의 시도가 필요할까?

먼저, 3x3 격자판의 전체 칸 수를 생각해보자. 3x3은 9개의 칸이 있겠지. 직사각형은 2x1이니까, 가로로는 3칸, 세로로는 1칸을 차지하겠지. 

...
2. **시도 방법**:    - 각 칸을 순회하며, 네가 해당 칸을 찍으면 직사각형이 덮인지 확인한다.    - 예를 들어, 첫 번째 시도: \( (1,1) \)을 찍으면 직사각형이 \( (1,1) \)과 \( (1,2) \)를 덮게 된다.    - 두 번째 시도: \( (1,2) \)을 찍으면 직사각형이 \( (1,2) \)과 \( (2,2) \)를 덮게 된다.    - 세 번째 시도: \( (2,1) \)을 찍으면 직사각형이 \( (2,1) \)과 \( (2,2) \)를 덮게 된다.    - 네 번째 시도: \( (2,2) \)을 찍으면 직사각형이 \( (2,2) \)와 \( (3,2) \)를 덮게 된다.

3. **검증**:
- 이러한 시도 방법을 통해 모든 가능한 직사각형 위치를 검토할 수 있다.
- 따라서, 최소 4번의 시도가 필요하다.

### 최종 답
\boxed{4}
---
</code></pre></div></div> <p><br><br></p> <hr> <h2 id="discussion">Discussion</h2> <p>During the course of the project, we derive the following N insights. Furthermore, techniques aim at enhancing reasoning capabilities, which emerges during the project, have been internalized as proprietary assets within the corporate.</p> <ul> <li>Utilizing overly difficult samples during the knowledge distillation phase can lead to performance degradation. This phenomenon is closely related to the capability of the backbone model. <ul> <li>For instance, the model trained on data distilled from <code class="language-plaintext highlighter-rouge">Qwen2.5-1.5B-Math-Instruct</code> demonstrates superior performance on mathematical benchmarks.</li> </ul> </li> <li> <p>Applying rule-based rewards through formatting reward functions initially leads to a <code class="language-plaintext highlighter-rouge">reduction in the number of generated tokens</code>, as the model prioritizes preserving the target format during <code class="language-plaintext highlighter-rouge">early training stages</code>. This trend gradually mitigates over time and is particularly pronounced compared to the model only undergoing knowledge distillation.</p> </li> <li>When enforcing rules to maintain <code class="language-plaintext highlighter-rouge">language consistency</code>, applying rewards <code class="language-plaintext highlighter-rouge">based on the proportion of tokens</code> generated during the “thinking process” tends to suppress the generation of mathematical equations or code blocks. <ul> <li>Even subtle differences in reward can lead to <code class="language-plaintext highlighter-rouge">reward hacking</code>, causing the model to converge on trivial solutions.</li> <li>This issue can arise even in the absence of ratio-based rewards if there is no rule to explicitly disregard equations or code blocks.</li> </ul> </li> <li>During reward model training, the presence or absence of reasoning data significantly impacts performance on general conversation tasks. <ul> <li>This phenomenon may indicate a limitation tied to the size of the reward model, warranting further investigation.</li> <li>In this project, rule-based rewards were applied to reasoning data (mathematics), and the reward model was trained excluding these samples.</li> </ul> </li> <li> <code class="language-plaintext highlighter-rouge">A greater number of rollouts per query</code> and larger batch sizes are effective for <code class="language-plaintext highlighter-rouge">improving model</code> performance and accelerating training. <ul> <li>Samples that are either too easy (i.e., correct answers across all rollouts for a given query) or too difficult (i.e., incorrect answers across all rollouts) do not contribute meaningfully to learning.</li> <li>Samples that are appropriately challenging—where the model can occasionally produce the correct answer within certain episodes—contribute significantly to performance improvements.</li> </ul> </li> </ul> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Dongjun Lee. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>