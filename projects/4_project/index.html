<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Domain Adaptation | Dongjun Lee </title> <meta name="author" content="Dongjun Lee"> <meta name="description" content="Training a large language model capable of understanding domain-specific task."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://dong-jun-2ee.github.io/projects/4_project/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Dongjun</span> Lee </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Domain Adaptation</h1> <p class="post-description">Training a large language model capable of understanding domain-specific task.</p> </header> <article> <h2 id="contents">Contents</h2> <ul> <li><a href="#summary">Summary</a></li> <li> <a href="#approches">Approches</a> <ul> <li><a href="#reading-comprehension">Reading Comprehension</a></li> <li><a href="#logit-swap">Logit Swap</a></li> </ul> </li> <li> <a href="#results">Results</a> <ul> <li><a href="#data-perspective">Data Perspective</a></li> <li><a href="#training-perspective">Training Perspective</a></li> </ul> </li> <li><a href="#discussion">Discussion</a></li> </ul> <p><br></p> <h2 id="summary">Summary</h2> <p>Open chat models and closed-source LLMs are primarily designed for general-purpose conversations. However, these models often lack sufficient knowledge in specific industrial domains and demonstrate significantly lower performance compared to their general conversational abilities. Consequently, there is a growing need to develop domain-specialized language models that possess knowledge tailored to specific industries—an endeavor that remains a highly challenging task.</p> <p>One of the most critical issues in domain adaptation is <em><code class="language-plaintext highlighter-rouge">catastrophic forgetting</code></em>, where a model loses previously acquired knowledge during additional training phases. Moreover, it struggles to integrate new domain-specific knowledge with existing general knowledge. Unlike retrieval-augmented generation (RAG) approaches, which improve performance by referencing external documents, domain-specialized models offer advantages in inference efficiency and reduced hallucination. To support this, various methods based on continual pre-training have been proposed.</p> <p>Related studies span multiple dimensions, including <code class="language-plaintext highlighter-rouge">data composition</code> [<a href="https://arxiv.org/abs/2309.09530" rel="external nofollow noopener" target="_blank">1</a>, <a href="https://arxiv.org/abs/2406.14491" rel="external nofollow noopener" target="_blank">2</a>, <a href="https://arxiv.org/abs/2311.08545" rel="external nofollow noopener" target="_blank">3</a>], <code class="language-plaintext highlighter-rouge">model architecture</code> [<a href="https://arxiv.org/abs/2401.02415" rel="external nofollow noopener" target="_blank">4</a>], and <code class="language-plaintext highlighter-rouge">training strategies</code> [<a href="https://arxiv.org/abs/2403.08763" rel="external nofollow noopener" target="_blank">5</a>, <a href="https://arxiv.org/abs/2406.17245" rel="external nofollow noopener" target="_blank">6</a>, <a href="https://arxiv.org/abs/2406.14833" rel="external nofollow noopener" target="_blank">7</a>].</p> <p>In our work, we focus on building a domain-specific language model through the lens of <em>data composition</em> and <em>training schemes</em>. Specifically, we adopt data composition methodologies originally designed for English to be effective for Korean. Through this adaptation, we achieve up to a <strong><em>10% performance improvement</em></strong> on domain-specific benchmarks while limiting performance degradation on general benchmarks to <strong><em>less than 1%</em></strong>.</p> <p>Furthermore, by employing a training technique leveraging <em>logit space manipulation</em>, we attain <strong><em>up to a 5% performance gain</em></strong> on domain-specific benchmarks.</p> <p><br></p> <h2 id="approaches">Approaches</h2> <ul> <li> <p>Reconstructing the dataset into reading comprehension format</p> </li> <li> <p>Accelerating the training through swapped-logit from teacher model</p> </li> </ul> <p><br></p> <h3 id="reading-comprehension">Reading Comprehension</h3> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/reading-comprehension-480.webp 480w,/assets/img/reading-comprehension-800.webp 800w,/assets/img/reading-comprehension-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/reading-comprehension.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="reading-comprehension" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> An overview of training documents formatted in a reading comprehension style </div> <p><br></p> <p>Building on previous research that demonstrated performance improvements by formatting raw domain-specific documents into a <a href="https://arxiv.org/abs/2309.09530" rel="external nofollow noopener" target="_blank"><em>reading comprehension</em></a> style according to predefined rules, we devised and applied a rule-based framework tailored for Korean-language documents.</p> <p>This approach utilizes conjunctions or specific phrases to identify patterns using regular expressions and reformats the documents to enhance their interpretability. For instance, in sentences containing the phrase <em><code class="language-plaintext highlighter-rouge">"is defined"</code></em>, we extract the term preceding the phrase along with the subsequent explanation, forming a <strong><em>term-definition pair</em></strong>. Similarly, when encountering conjunctions such as <em><code class="language-plaintext highlighter-rouge">"but"</code></em> or <em><code class="language-plaintext highlighter-rouge">"however"</code></em>, we infer that the preceding and following sentences express <strong><em>contrasting meanings</em></strong>.</p> <p>By applying such rules, we concatenate the reformulated information to the end of the original document. This enables the model to extract and learn key information more efficiently, even from a limited number of training samples.</p> <p>The <em><code class="language-plaintext highlighter-rouge">reading comprehension</code></em> framework can encompass various task types, including:</p> <ul> <li> <strong>Summarization</strong>: defines the relationship between the title (usually the first sentence) and the main body of the text.</li> <li> <strong>Word-to-Text</strong>: generates question-answer pairs based on the definition of a word.</li> <li> <strong>Natural Language Inference</strong>: bridges an answer by identifying the logical relationship between two sentences.</li> <li> <strong>Commonsense Reasoning</strong>: explains causal relationships by generating question-answer pairs based on cause-and-effect pairs.</li> <li> <strong>Paragraph Detection</strong>: identifies the relationship between two sentences, such as whether they convey the same or different meanings.</li> <li> <strong>Text Completion</strong>: converts a sentence into a question-answer pair by arbitrarily cutting out the middle of the sentence to allow for completion.</li> </ul> <p>We extract a total of 825,773 instruction-response pairs from 383,580 legal documents.</p> <p>Detailed information related to the extracted tasks is provided in the table below.</p> <table> <thead> <tr> <th>Category</th> <th>Subcategory</th> <th>Count</th> </tr> </thead> <tbody> <tr> <td>Summarize</td> <td>Summarization</td> <td>200,093</td> </tr> <tr> <td>Word2text</td> <td>word2text</td> <td>98,941</td> </tr> <tr> <td> </td> <td>definition</td> <td>857</td> </tr> <tr> <td>NLI</td> <td>True</td> <td>132,809</td> </tr> <tr> <td> </td> <td>False</td> <td>54,389</td> </tr> <tr> <td> </td> <td>Possibly true</td> <td>37,245</td> </tr> <tr> <td>Common reason</td> <td>Cause and Effect</td> <td>134,075</td> </tr> <tr> <td> </td> <td>Effect and Cause</td> <td>14,913</td> </tr> <tr> <td>Paraphrase</td> <td>Different</td> <td>44,409</td> </tr> <tr> <td> </td> <td>Similar</td> <td>27,769</td> </tr> <tr> <td>Text Completion</td> <td>Text Completion</td> <td>80,273</td> </tr> <tr> <td>Total</td> <td>-</td> <td>383,580</td> </tr> </tbody> </table> <p><br><br></p> <p>Example:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">본문</span><span class="p">:</span> <span class="n">그런데</span> <span class="n">A에게는</span> <span class="n">대전에서</span> <span class="n">근무하는</span> <span class="n">아들</span> <span class="n">乙이</span> <span class="n">있고</span> <span class="p">,</span> <span class="n">乙이</span> <span class="n">위</span> <span class="n">차량을</span> <span class="n">대전에서</span> <span class="n">운전하고</span> <span class="n">주말에는</span> <span class="n">서울에</span> <span class="n">올라오는</span> <span class="n">등</span> <span class="n">차량의</span> <span class="n">주운전</span><span class="bp">...</span>
<span class="mi">2</span> <span class="n">위</span> <span class="n">법률</span> <span class="n">도메인</span> <span class="n">본문에</span> <span class="n">대해</span> <span class="n">몇</span> <span class="n">가지</span> <span class="n">질문에</span> <span class="n">답해주세요</span><span class="p">:</span>

<span class="n">문맥에서</span> <span class="n">다음</span> <span class="n">가설을</span> <span class="n">도출할</span> <span class="n">수</span> <span class="n">있나요</span><span class="err">?</span>
<span class="n">문맥</span><span class="p">:</span> <span class="n">이에</span> <span class="n">甲은</span> <span class="n">乙에게</span> <span class="n">흠이</span> <span class="n">없는</span> <span class="n">새</span> <span class="n">승용차로</span> <span class="n">교환하여</span> <span class="n">줄</span> <span class="n">것을</span> <span class="n">요구하였다</span><span class="p">.</span> <span class="bp">...</span>
<span class="n">가설</span><span class="p">:</span> <span class="n">乙은</span> <span class="n">甲과</span> <span class="n">매매계약을</span> <span class="n">체결할</span> <span class="n">당시에</span> <span class="n">사용한</span> <span class="n">자동차매매약관에</span> <span class="n">다음과</span> <span class="n">같은</span> <span class="n">조항이</span> <span class="n">있음을</span> <span class="n">이유로</span> <span class="n">甲의</span> <span class="n">요구를</span> <span class="n">거절하였다</span> <span class="p">.</span> <span class="sh">"</span><span class="s"> ( 자동...
답변: 두 문장은 모순의 관계를 갖고 있습니다.

2. 약관의 법적 규제 (1) 전술한 약관의 부정적인 면을 주로 규제하기 위해 일찍이 외국의 입법례는 법률로써 이를 규율하는 방식을 취..
위 문장의 나머지 부분을 제공하세요: 대표적인 것으로 , 독일의 「보통거래약관에 관한 법률」 (Gesetz zur Regelung des Rechts de...
</span><span class="sh">"</span><span class="n">이에</span> <span class="n">甲은</span> <span class="n">乙에게</span> <span class="n">흠이</span> <span class="n">없는</span> <span class="n">새</span> <span class="n">승용차로</span> <span class="n">교환하여</span> <span class="n">줄</span> <span class="n">것을</span> <span class="n">요구하였다</span> <span class="p">.</span> <span class="sh">"</span><span class="s">의 의미를 반박하는 문장을 작성할 수 있나요? 乙은 甲과 매매계...
이 법률 단락에 대한 제목을 생성하세요.

제목: 제3절 約款에 의한 契約의 成立 사례
</span></code></pre></div></div> <p><br></p> <h3 id="logit-swap">Logit Swap</h3> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mix-cpt-480.webp 480w,/assets/img/mix-cpt-800.webp 800w,/assets/img/mix-cpt-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/mix-cpt.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="mix-cpt" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Training framework of logit swap method. </div> <p><br></p> <p>Inspired by prior research on <a href="https://openreview.net/forum?id=h1XoHOd19I" rel="external nofollow noopener" target="_blank">domain adaptation</a> using <code class="language-plaintext highlighter-rouge">knowledge distillation</code>, we explore domain adaptation for Korean language models. In particular, we aim to <strong><em>maintain a balance between existing and newly acquired knowledge</em></strong> to mitigate <code class="language-plaintext highlighter-rouge">catastrophic forgetting</code>.</p> <p>The proposed method is motivated by knowledge distillation from a teacher model. Notably, the approach allows flexibility in selecting or modifying the teacher model. When a high-capability teacher is used, the model benefits not only from domain-specific knowledge but also from general knowledge because it is trained on logit distribution. Furthermore, our method is applicable to <strong><em>both the continual pre-training stage and the supervised fine-tuning stage</em></strong>, offering additional versatility.</p> <p>We consider a high-performing general-domain model as the teacher and compute the logit distribution over the training data. If the real target token does not match the token with the maximum logit value, we apply a simple trick by swapping them. <code class="language-plaintext highlighter-rouge">This trick encourages the teacher model to produce a logit distribution more similar with domain expert.</code> Proposed approach assumes that the teacher model’s output distribution is not significantly different from the optimal one.</p> <p>Through this strategy, the student model benefits from a distillation-like process, as if guided by a virtual domain expert, while also learning from <strong><em>soft supervision signals, thereby alleviating the problem of forgetting</em></strong>.</p> <p><br></p> <h2 id="results">Results</h2> <h3 id="data-perspective">Data perspective</h3> <table> <thead> <tr> <th>Models</th> <th>EN - general</th> <th>Ko - general</th> <th>Ko - legal</th> </tr> </thead> <tbody> <tr> <td>Basemodel</td> <td>61.44%</td> <td>39.69%</td> <td>30.67%</td> </tr> <tr> <td>Continual Pre-training on raw document</td> <td><strong> 70.60% </strong></td> <td>45.39%</td> <td>31.29%</td> </tr> <tr> <td>Continual Pre-training on reading comprehension data</td> <td>70.42%</td> <td><strong> 48.13% </strong></td> <td><strong> 33.07% </strong></td> </tr> </tbody> </table> <div class="caption"> Overall performance of pretrained model </div> <p><br></p> <table> <thead> <tr> <th>Models</th> <th>EN - general</th> <th>Ko - general</th> <th>En - legal</th> <th>Ko - legal</th> </tr> </thead> <tbody> <tr> <td>General Model</td> <td>76.59%</td> <td>55.92%</td> <td>34.26%</td> <td>39.16%</td> </tr> <tr> <td>Ours</td> <td><strong> 77.39% </strong></td> <td><strong> 58.00% </strong></td> <td><strong> 52.73% </strong></td> <td><strong> 58.16% </strong></td> </tr> </tbody> </table> <div class="caption"> Performance comparison between General chat model and our domain-specific model </div> <p><br></p> <p>According to the experimental results, the model trained on legal documents outperforms the base model on domain-specific tasks. Furthermore, the model that undergoes continual pre-training demonstrates superior performance even in general-domain settings, <strong><em>suggesting that the nature of legal texts—which often require complex reasoning—contributes to this enhancement.</em></strong> Notably, documents augmented through reading comprehension tasks lead to greater performance gains compared to model trained on raw corpora. These findings indicate that the reading <code class="language-plaintext highlighter-rouge">comprehension approach effectively extracts informative signals from limited data</code>, emphasizing its worth in improving performance from a data composition perspective.</p> <p><br></p> <h3 id="training-perspective">Training perspective</h3> <table> <thead> <tr> <th>Models</th> <th>General</th> <th>Legal</th> </tr> </thead> <tbody> <tr> <td>Baseline</td> <td>62.47%</td> <td>43.19%</td> </tr> <tr> <td>Ours</td> <td>61.88%</td> <td>45.73%</td> </tr> </tbody> </table> <div class="caption"> Overall performance of our approach compared to the baseline model trained on domain data without logit swap </div> <p><br></p> <p>We evaluate the efficiency and effectiveness of our proposed <strong><em>logit swap</em></strong> approach by comparing it with a baseline model trained using standard cross-entropy loss.</p> <p>While our model underperforms the baseline in the general domain, it achieves superior performance in the legal domain. Through experiments, we observe that using a stronger teacher model leads to improved performance on domain-specific benchmarks, with only a marginal decrease in general-domain performance. This suggests that greater performance gains may be achievable when more computational resources are available.</p> <p>However, our approach still exhibits limitations, including <code class="language-plaintext highlighter-rouge">relatively modest performance improvements</code> and <code class="language-plaintext highlighter-rouge">minimal, yet non-negligible, degradation</code> in general-domain performance.</p> <p><br></p> <h2 id="discussion">Discussion</h2> <p>During the course of this project, we developed simple yet effective domain adaptation approaches from both the perspective of data composition and training algorithms. Our primary focus was on training domain-specific large language models (LLMs), aiming to efficiently acquire domain knowledge from constrained environments while minimizing catastrophic forgetting.</p> <p>This project was conducted with a specific focus on the legal domain. Due to the reliance on internal training and benchmark datasets, the work faces limitations regarding public accessibility. Additionally, both the proposed methods and source code are not publicly available.</p> <ul> <li>We observed that modifying data composition can significantly enhance the efficiency of domain adaptation. <ul> <li>In this context, our rule-based approach, though simple, proved to be effective. We found that overly <strong><em>strict rules can hinder the generation of meaningful learning signals</em></strong>, whereas <strong><em>overly lenient rules tend to promote duplication of documents</em></strong>.</li> <li>Similar to prior research, structuring data in the format of reading comprehension tasks contributed to <strong><em>improvements in prompting capabilities</em></strong>, which in turn enhanced instruction-following performance after supervised fine-tuning.</li> </ul> </li> <li> <p>When conducting continual pre-training on datasets composed of legal reasoning data, we observed that the model’s performance on general domain tasks also improved.</p> </li> <li>The logit swap technique, despite its simplicity, contributed meaningfully to improving domain adaptation performance. <ul> <li>The effectiveness of this method scales with the capabilities of the teacher model; stronger teacher models lead to greater performance gains, and the inverse also holds.</li> <li>However, this approach still presents <code class="language-plaintext highlighter-rouge">limitations</code>, as it does <code class="language-plaintext highlighter-rouge">not deliver outstanding</code> adaptation performance and the <code class="language-plaintext highlighter-rouge">modest performance degradation</code> it may introduce.</li> </ul> </li> </ul> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Dongjun Lee. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>