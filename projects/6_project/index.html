<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Knowledge Distillation | Dongjun Lee </title> <meta name="author" content="Dongjun Lee"> <meta name="description" content="Transferring the capabilities of a large language model to a smaller one."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://dong-jun-2ee.github.io/projects/6_project/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Dongjun</span> Lee </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Knowledge Distillation</h1> <p class="post-description">Transferring the capabilities of a large language model to a smaller one.</p> </header> <article> <h2 id="contents">Contents</h2> <ul> <li><a href="#summary">Summary</a></li> <li> <a href="#methodology">Methodology</a> <ul> <li><a href="#overview">Overview</a></li> <li><a href="#learning-from-teacher">Learning from teacher</a></li> <li><a href="#training-the-classmate">Training the classmate</a></li> <li><a href="#learning-from-strong-and-weak-models">Learning from strong and weak models</a></li> </ul> </li> <li><a href="#performance-comparisons">Performance Comparsions</a></li> <li><a href="#discussion">Discussion</a></li> </ul> <p><br></p> <h2 id="summary">Summary</h2> <p>Recent efforts have focused on training foundation models more <strong>efficiently and effectively</strong>. To this end, both classical approaches such as <em>knowledge distillation</em> and more sophisticated techniques have been explored. For example, <a href="https://arxiv.org/abs/2503.19786" rel="external nofollow noopener" target="_blank">Gemma 3</a> was distilled from <a href="https://gemini.google.com/" rel="external nofollow noopener" target="_blank">Gemini</a> usning thier improved approach (<a href="https://arxiv.org/abs/2306.13649" rel="external nofollow noopener" target="_blank">GKD</a>), and the <a href="https://arxiv.org/abs/2407.21783" rel="external nofollow noopener" target="_blank">LLaMA series</a> also maximized training efficiency through distillation from larger models. Particularly, as the importance of <em>reasoning models</em> has grown, this approach has been adopted in models like <a href="https://arxiv.org/abs/2505.09388" rel="external nofollow noopener" target="_blank">Qwen3</a> and <a href="https://arxiv.org/abs/2501.12948" rel="external nofollow noopener" target="_blank">DeepSeek</a>, demonstrating its effectiveness.</p> <p><a href="https://arxiv.org/abs/1503.02531" rel="external nofollow noopener" target="_blank">Knowledge Distillation</a>, initially developed for vision models, has proven effective for language models as well. In the era of large language models (LLMs), two main types of distillation have emerged: <strong>white-box</strong> and <strong>black-box</strong>. Among these, white-box distillation is applicable when the teacher model’s logits are observable. This method has recently evolved in close connection with reinforcement learning [<a href="https://arxiv.org/abs/2306.13649" rel="external nofollow noopener" target="_blank">1</a>, <a href="https://arxiv.org/abs/2306.08543" rel="external nofollow noopener" target="_blank">2</a>, <a href="https://arxiv.org/abs/2402.03898" rel="external nofollow noopener" target="_blank">3</a>].</p> <p>Despite these advances, existing methods still operate under the <strong>strong-to-weak distillation paradigm</strong>. We observe that there remains significant room for improvement. Inspired by the concept of <a href="https://arxiv.org/abs/2406.14629" rel="external nofollow noopener" target="_blank">Weak-to-Strong Generalization</a>, we propose a new approach that leverages <em>additional signals</em> from weak models to further enhance performance.</p> <p>Our method introduces a framework in which feedback is generated from a <strong>weaker model of the same size</strong>. Rather than treating this feedback as reliable supervision, we treat it as <strong>noisy or incorrect information</strong> and train the model accordingly. This allows the learning process to benefit from diverse and challenging signals.</p> <p>Our approach achieves <strong>over 3% improvement</strong> compared to existing state-of-the-art models and shows <strong>superior performance</strong> on 4 out of 5 benchmarks.</p> <p><br></p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/distillation-480.webp 480w,/assets/img/distillation-800.webp 800w,/assets/img/distillation-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/distillation.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="distillation" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Overall pipeline for `Class knowledge distillation` </div> <h2 id="methodology">Methodology</h2> <ul> <li> <p>Overview</p> </li> <li> <p>Learning from teacher</p> </li> <li> <p>Training the classmate</p> </li> <li> <p>Learning from strong and weak model</p> </li> </ul> <p><br></p> <h3 id="overview">Overview</h3> <p>We propose a novel learning approach that not only facilitates knowledge transfer from a strong model (referred to as the <strong><em>teacher</em></strong>) but also leverages models that are smaller or exhibit lower performance than the target model (referred to as the <strong><em>student</em></strong>) to enhance performance.</p> <p>We conceptualize this learning mechanism as analogous to how a student acquires knowledge in a classroom setting—not only from a teacher but also from peers—thus, we term this approach <em><code class="language-plaintext highlighter-rouge">class knowledge distillation</code></em>. In this context, the lower-performing models are referred to as <strong><em>classmate models</em></strong>.</p> <p>Our method employs an RL-like approach, where feedback is received from the teacher based on sentences generated in an on-policy manner. Simultaneously, feedback is also obtained from classmate models; however, this feedback is regarded as potentially incorrect and is therefore termed <em><code class="language-plaintext highlighter-rouge">misfeedback</code></em>. The learning objective is to diverge from the sentence distribution of the classmate models while converging towards that of the teacher, following a paradigm similar to contrastive learning.</p> <p><br></p> <h3 id="learning-from-teacher">Learning from teacher</h3> <p>We conduct training using a fixed instruction-following dataset $(x, y) \in \mathcal{D}$, where $x$ denotes a user prompt and $y$ denotes the corresponding assistant response.</p> <p>Training is performed using a static dataset by computing token-level logits from both a teacher model and a student model, and minimizing the Kullback–Leibler divergence between their output distributions (<code class="language-plaintext highlighter-rouge">off-policy distillation</code>). This process can be formulated as follows:</p> \[L_{\text{off}}(p, q_\theta) = \mathbb{E}_{\mathbf{x}} \mathbb{E}_{\mathbf{y} \sim p(\cdot|\mathbf{x})} \left[ \log \frac{p(\mathbf{y}|\mathbf{x})}{q_\theta(\mathbf{y}|\mathbf{x})} \right]\] \[= \frac{1}{|\mathcal{D}|} \sum_{(\mathbf{x}, \mathbf{y}) \in \mathcal{D}} p(\mathbf{y}|\mathbf{x}) \log \frac{p(\mathbf{y}|\mathbf{x})}{q_\theta(\mathbf{y}|\mathbf{x})}\] \[= \frac{1}{|\mathcal{D}|} \sum_{\mathbf{x} \in \mathcal{D}_\mathbf{X}} \sum_t^{|\mathbf{y}|} \sum_{y_t \in V} p(y_t | \mathbf{y}_{&lt;t}, \mathbf{x}) \log \frac{p(y_t | \mathbf{y}_{&lt;t}, \mathbf{x})}{q_\theta(y_t | \mathbf{y}_{&lt;t}, \mathbf{x})}\] <p>To <code class="language-plaintext highlighter-rouge">mitigate the training–inference mismatch</code>, also known as exposure bias, we further incorporate <strong><em>student-generated outputs</em></strong> (SGO) into the training process. In this setting—analogous to <code class="language-plaintext highlighter-rouge">on-policy training</code> in reinforcement learning—the student model generates a response for a given prompt, and distillation is performed using the teacher model’s logits corresponding to the generated sequence. The procedure can be expressed as:</p> \[L_{on}(p, q_\theta) = \mathbb{E}_{x \sim D} \left[ \mathbb{E}_{y \sim q_\theta(\cdot \mid x)} \left[ \log \frac{p(\mathbf{y}|\mathbf{x})}{q_\theta(\mathbf{y}|\mathbf{x})} \right] \right]\] <p>In this context, it is important to note that the choice of loss function can have a significant impact on model performance.</p> <p><br></p> <h3 id="training-the-classmate">Training the classmate</h3> <p>In addition to training the student model, we simultaneously train a classmate model. Since a poorly trained classmate may provide low-quality or misleading feedback (i.e., misfeedback), leading to meaningless learning signals, we adopt a training strategy that allows the classmate to gradually improve over time.</p> <p>Notably, our empirical results suggest that <strong><em>the degree to which the classmate is trained plays a crucial role</em></strong> in achieving effective <code class="language-plaintext highlighter-rouge">weak-to-strong generalization</code>. Furthermore, the ability to precisely control the training dynamics of the classmate is a key factor in maximizing the effectiveness of our proposed framework.</p> <p>The classmate is trained under a loss function analogous to that used for the student model. However, the classmate’s generated samples are not used for training. Instead, it learns solely from the teacher’s supervision. This design choice is made to prevent the classmate from becoming overly dominant in performance.</p> <p>The objective function for the classmate model is defined as:</p> \[L_{\text{classmate}}(p, c_\phi) = \mathbb{E}_{x \sim D} \left[ \mathbb{E}_{\mathbf{y} \sim p(\cdot|\mathbf{x})} \left[ \log \frac{p(\mathbf{y}|\mathbf{x})}{c_\phi(\mathbf{y}|\mathbf{x})} \right] \right]\] <p>where $c_\phi$ denotes the classmate model parameterized by $\phi$.</p> <p><br></p> <h3 id="learning-from-weak-models">Learning from weak models</h3> <p>We propose a learning approach in which the student model is trained using feedback from a classmate model that performs worse than the student, but not to the extent of significantly underperforming. This setup bears resemblance to <strong><em>contrastive learning objectives</em></strong> or <strong><em>Direct Preference Optimization</em></strong> <a href="https://arxiv.org/abs/2305.18290" rel="external nofollow noopener" target="_blank">(DPO)</a>.</p> <p>Intuitively, during student-generated output (SGO) training, the classmate provides feedback in a manner analogous to the teacher model. However, instead of directly following this feedback, the student is trained to resist or <code class="language-plaintext highlighter-rouge">apart from the misfeedback provided by the classmate</code>.</p> <p>This training paradigm can be viewed as analogous to DPO, where the objective is to decrease the likelihood of generating rejected samples. Similarly, it aligns with the principle of contrastive learning, where representations are encouraged to move away from negative samples in the latent space — or, in our context, the sentence space.</p> \[L_{\text{misfeedback}}(q_\theta, c) = - \mathbb{E}_{x \sim D} \left[ \mathbb{E}_{\mathbf{y} \sim q_\theta(\cdot|\mathbf{x})} \left[ \log \frac{q_\theta(\mathbf{y}|\mathbf{x})}{c(\mathbf{y}|\mathbf{x})} \right]\right]\] <p><br></p> <h2 id="performance-comparisons">Performance Comparisons</h2> <table> <thead> <tr> <th>Models</th> <th>Dolly</th> <th>Self-inst</th> <th>Vicuna</th> <th>Super-Natural</th> <th>Unnatural</th> <th>AVG</th> </tr> </thead> <tbody> <tr> <td>GKD</td> <td>23.75</td> <td>12.73</td> <td>16.64</td> <td>26.05</td> <td>27.70</td> <td>21.37</td> </tr> <tr> <td>MiniLLM</td> <td>23.84</td> <td>12.44</td> <td>18.29</td> <td>22.62</td> <td>23.26</td> <td>20.09</td> </tr> <tr> <td>DistiLLM</td> <td>26.11</td> <td>13.14</td> <td><strong> 18.46 <strong></strong></strong></td> <td>27.51</td> <td>30.11</td> <td>23.07</td> </tr> <tr> <td>Ours</td> <td><strong> 27.02 </strong></td> <td><strong> 13.68 </strong></td> <td>17.41</td> <td><strong> 28.51 </strong></td> <td><strong> 30.26</strong></td> <td><strong> 23.38 </strong></td> </tr> </tbody> </table> <p><br></p> <p>We conduct a comparative evaluation of our CKD method against several state-of-the-art distillation techniques on five benchmark datasets.</p> <ul> <li>Dolly: the 500-sample test set we split from the databricks-dolly-15k dataset.</li> <li>Self-inst: A user-oriented instruction-following set with 252 samples.</li> <li>Vicuna: The 80 challenging questions used in the Vicuna evaluation.</li> <li>Super-Natural: : The test set of SUPER-NATURALINSTRUCTIONS consisting of 9K samples ranging from 119 tasks.</li> <li>Unnatural: We randomly sample 10K samples from the core set of UNNATURALINSTRUCTIONS for evaluation.</li> </ul> <p>We adopt the <code class="language-plaintext highlighter-rouge">Rouge-L</code> score to evaluate the model-generated response.</p> <p>Firstly, we observed that our knowledge distillation method achieved the highest performance across all datasets, with the exception of a single benchmark.</p> <p>Although our model was trained solely on the Dolly training dataset, its consistently strong performance across diverse datasets demonstrates its <strong><em>robust generalization ability</em></strong>.</p> <p>At the same time, our findings provide evidence that weak-to-strong generalization is also applicable in the context of knowledge distillation. To the best of our knowledge, this is the first such observation reported at the <code class="language-plaintext highlighter-rouge">sentence level in knowledge distillation</code>.</p> <p>Moreover, it is important to note that all other models perform distillation from supervised fine-tuned models, meaning that our method does not incur a significant increase in computational budget.</p> <p>Notably, the <strong>baseline methods considered are orthogonal to our training framework</strong>. As such, ideas from approaches like MiniLLM and DistiLLM can be incorporated into our framework, potentially leading to further improvements in performance.</p> <p><br></p> <h2 id="discussion">Discussion</h2> <p>This work is part of an <strong><em>ongoing project</em></strong> and is <strong><em>subject to continuous updates and revisions</em></strong>. Furthermore, we note that the approach is intended for internal use within a corporate aimed at building compact yet powerful models, and is not intended for external release.</p> <ul> <li>We deviate from the traditional strong-to-weak knowledge distillation paradigm and observe that <code class="language-plaintext highlighter-rouge">weak-to-strong distillation</code> can effectively improve model performance. <ul> <li>This phenomenon can be interpreted similarly to contrastive learning, suggesting that negative samples or <strong><em>negative feedback may serve as valuable learning signals</em></strong>.</li> <li>In particular, we find that the effectiveness of negative samples is highly sensitive to their difficulty: <code class="language-plaintext highlighter-rouge">overly easy (easy negatives) or overly hard (hard negatives) samples can adversely affect training</code>. Careful control over this difficulty is therefore crucial.</li> </ul> </li> <li>We also observe that leveraging SGO during training significantly contributes to performance improvements. Notably, <strong><em>even when pool samples are generated in early epochs, meaningful learning signals can still be extracted through the use of classmate models</em></strong>. <ul> <li>This suggests a potential advantage over methods such as DistiLLM or MiniLLM, which aim to mitigate the negative impact of low-quality samples generated in early training stages. Our findings indicate that such early samples may instead offer opportunities for improvement when properly utilized.</li> </ul> </li> </ul> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Dongjun Lee. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>